###################################################################################################
####### This script we only consider the first stage, i.e. finding change point candidates. #######
###################################################################################################
rm(list = ls(all = TRUE))
#setwd("~/Dropbox (Personal)/Project 3/Numerical Experiments")
data <- read.csv("Weekly.csv", header = TRUE)

####### Pre-processing data by removing NA columns and set up time stamps #######
library(dplyr)
time.stamp <- as.POSIXlt(data[,1], format = "%m/%d/%Y")
pre.data <- as.matrix(data[,-1] %>% 
    select_if(~ !any(is.na(.))))

####### Using algorithm to analyze change points #######
source("functions_SBDetection.R")
###### FISTA estimating functions ######
fista.nuclear <- function(A, b, lambda, d, niter, backtracking = TRUE, phi.true){
    tnew = t <- 1
    x <- matrix(0, d, d)
    xnew <- x
    y <- x
    AtA <- t(A) %*% A
    Atb <- t(A) %*% b
    
    obj.val = rel.err <- c()
    if(backtracking == TRUE){
        L <- norm(A, "2")^2 / 5
        eta <- 2
    }else{
        L <- norm(A, "2")^2
    }
    for(i in 1:niter){
        if(backtracking == TRUE){
            L.bar <- L
            flag <- FALSE
            while(flag == FALSE){
                prox <- prox.nuclear.func(y, A, b, L.bar, lambda, AtA, Atb)
                
                ### restricted part
                for(rr in 1:d){
                    for(cc in 1:d){
                        if(abs(prox[rr,cc]) > 0.25){
                            prox[rr,cc] <- 0.25*sign(prox[rr,cc])
                        }
                    }
                }
                
                if(f.func(prox, A, b) <= Q.func(prox, y, A, b, L.bar, AtA, Atb)){
                    flag <- TRUE
                }else{
                    L.bar <- L.bar * eta
                }
            }
            L <- L.bar
        }
        x <- xnew
        xnew <- prox
        t <- tnew
        tnew <- (1 + sqrt(1 + 4*t^2)) / 2
        y <- xnew + ((t - 1) / tnew) * (xnew - x)
        
        obj.val <- c(obj.val, f.func(xnew, A, b) + nuclear.pen(xnew, lambda))
        rel.err <- c(rel.err, norm(xnew - phi.true, "F") / norm(phi.true, "F"))
    }
    return(list(phi.hat = xnew, obj.vals = obj.val, rel.err = rel.err))
}
fista.sparse <- function(A, b, lambda, d, niter, backtracking = TRUE, phi.true){
    tnew = t <- 1
    x <- matrix(0, d, d)
    xnew <- x
    y <- x
    AtA <- t(A) %*% A
    Atb <- t(A) %*% b
    
    obj.val = rel.err <- c()
    if(backtracking == TRUE){
        L <- norm(A, "2")^2 / 5
        eta <- 2
    }else{
        L <- norm(A, "2")^2
    }
    for(i in 1:niter){
        if(backtracking == TRUE){
            L.bar <- L
            flag <- FALSE
            while(flag == FALSE){
                prox <- prox.sparse.func(y, A, b, L.bar, lambda, AtA, Atb)
                if(f.func(prox, A, b) <= Q.func(prox, y, A, b, L.bar, AtA, Atb)){
                    flag <- TRUE
                }else{
                    L.bar <- L.bar * eta
                }
            }
            L <- L.bar
        }
        x <- xnew
        xnew <- prox
        t <- tnew
        tnew <- (1 + sqrt(1 + 4*t^2)) / 2
        y <- xnew + ((t - 1) / tnew) * (xnew - x)
        
        obj.val <- c(obj.val, f.func(xnew, A, b) + sparse.pen(xnew, lambda))
        rel.err <- c(rel.err, norm(xnew - phi.true, "F") / norm(phi.true, "F"))
    }
    return(list(phi.hat = xnew, obj.vals = obj.val, rel.err = rel.err))
}
shrinkage <- function(y, tau){
    z <- matrix(0, nrow = nrow(y), ncol = ncol(y))
    for(i in 1:nrow(y)){
        for(j in 1:ncol(y)){
            z[i,j] <- sign(y[i,j]) * max(abs(y[i,j]) - tau, 0)
        }
    }
    return(z)
}
shrinkage.lr <- function(y, tau){
    z <- rep(0, length(y))
    for(i in 1:length(y)){
        z[i] <- sign(y[i]) * max(0, abs(y[i]) - tau)
    }
    return(z)
}
f.func <- function(x, A, b){
    return(0.5 * norm(A %*% x - b, "F")^2)
}
gradf.func <- function(x, AtA, Atb){
    return(AtA %*% x - Atb)
}
nuclear.pen <- function(x, lambda){
    d <- svd(x)$d
    return(lambda * sum(d))
}
sparse.pen <- function(x, lambda){
    return(lambda*sum(x))
}
Q.func <- function(x, y, A, b, L, AtA, Atb){
    return(f.func(y, A, b) + sum((x - y) * gradf.func(y, AtA, Atb)) + 0.5 * L * norm(x - y, "F")^2)
}
prox.nuclear.func <- function(y, A, b, L, lambda, AtA, Atb){
    Y <- y - (1 / L) * gradf.func(y, AtA, Atb)
    d <- shrinkage.lr(svd(Y)$d, 2*lambda / L)
    return(svd(Y)$u %*% diag(d) %*% t(svd(Y)$v))
}
prox.sparse.func <- function(y, A, b, L, lambda, AtA, Atb){
    Y <- y - (1 / L) * gradf.func(y, AtA, Atb)
    return(shrinkage(Y, 2*lambda / L))
}
obj.func <- function(x.lr, x.sparse, A, b, lambda, mu){
    ### x.sparse is a list
    m <- length(x.sparse)
    loss <- 0
    for(i in 1:m){
        loss <- loss + f.func((x.lr + x.sparse[[i]]), A, b) + sparse.pen(x.sparse[[i]], lambda)
    }
    return(loss + nuclear.pen(x.lr, mu))
}

###### Helper functions ######
bic.sparse <- function(X, Y, lambda.sel.vec, L.hat){
    T.new <- dim(Y)[1] + 1
    bic.seq <- rep(0, length(lambda.sel.vec))
    S.hat.lst <- vector("list", length(lambda.sel.vec))
    for(ll in 1:length(lambda.sel.vec)){
        lambda.sel <- lambda.sel.vec[ll]
        fit.S <- fista.sparse(X, Y, lambda = lambda.sel, k, niter = 100, backtracking = TRUE, diag(k))
        S.hat <- t(fit.S$phi.hat)
        S.hat.lst[[ll]] <- S.hat
        
        phi.hat <- L.hat + S.hat
        data.pred <- X %*% t(phi.hat)
        residual <- Y - data.pred
        sigma.hat <- 0*diag(k)
        for(t in 1:(T.new-1)){
            sigma.hat <- sigma.hat + residual[t,] %*% t(residual[t,])
        }
        sigma.hat <- (1/T.new) * sigma.hat
        ee.temp <- min(eigen(sigma.hat)$values)
        if(ee.temp <= 0){
            sigma.hat <- sigma.hat + 2*(abs(ee.temp) + 1e-4 * diag(k))
        }
        log.det <- log(det(sigma.hat) + 0*1e-10)
        ### l_0 norm for sparse vector/matrix
        count <- 0
        for(row in 1:k){
            for(col in 1:k){
                if(S.hat[row, col] != 0){
                    count <- count + 1
                }
            }
        }
        
        bic.seq[ll] <- log.det + log(T.new)*(count / T.new)
        # print(paste(ll, "++++++++++++++++++++++"))
    }
    idx <- which.min(bic.seq)
    S.hat <- S.hat.lst[[idx]]
    lambda <- lambda.sel.vec[idx]
    return(list(S.hat = S.hat, bic = bic.seq, lambda = lambda))
}
########################################################
library("igraph")
library("ggplot2")
library("glmnet")
library("matrixcalc")
library("fields")
library("vars")
library("MTS")
library("mvtnorm")
library("xtable")
library("lattice")
library("doParallel")

######## First step: Fused lasso change point detection, find candidates #########
### Variable related parameters
T <- dim(pre.data)[1]
k <- dim(pre.data)[2]
X.sel <- as.matrix(pre.data[1:(T-1),])
Y.sel <- as.matrix(pre.data[2:T,])

### Generating clusters to parallel computing
cl <- makeCluster(2)
registerDoParallel(cl)
getDoParWorkers()

####################################
######## Setting parameters ########
####################################
p = p.t <- 1                ### true AR lag
tol.sel <- 0.1
rep <- 0
tol <- 5*10^(1)             ### tolerance 
step.size <- 2*10^(-4)      ### step size 
max.iteration <- 100        ### max number of iteration for the Lasso solution
max.iter <- 10
method <- c('LASSO')

### estimation tuning
# mu.sel <- 2.5     ### 1-factor
# mu.sel <- 0.75    ### 3-factor
mu.sel <- 0.50    ### 5-factor

### Tuning parameter for first step
l.init <- (T*log(k))^0.5
lambda.1.cv <- 0.0001*seq(l.init, 9*l.init, l.init/2)
blocks <- seq(0, T, floor(T/(floor(2*sqrt(T)))))
n.new <- length(blocks) - 1
blocks.size <- sapply(c(1:n.new), function(jjj) blocks[jjj+1] - blocks[jjj] )
bbb <- floor(n.new/5)
aaa <- sample(1:5, 1)

initial.phi <- matrix(0, k, k*p*(n.new))       ### this is the inital value for S_j's in our L+S_j paper which you can set from previous observations
cv.index <- seq(aaa, n.new, floor(n.new/bbb))  ### cv index, for cross validation index.

### Initialization for low-rank component
fit.initL <- fista.nuclear(X.sel, Y.sel, lambda = mu.sel, k, niter = 100, backtracking = TRUE, 0*diag(k))
L.new <- fit.initL$phi.hat
qr(L.new)$rank
print(plot.matrix(abs(L.new), 1))

#######################################################################################
################################### Naive Algorithm ###################################
#######################################################################################
###### Step 1 ######
### removing estimated low-rank effects from original data
# dat.sel <- matrix(0, T, k); dat.sel[1,] <- pre.data[1,]
# Y.tmp <- matrix(0, T-1, k); Y.tmp <- pre.data[(2:T),]
# for(j in 1:(T-1)){
#     xtm <- matrix(dat.sel[j,], 1, k)
#     ytm <- matrix(Y.tmp[j,], 1, k)
#     tmp <- ytm - xtm %*% t(L.new)
#     dat.sel[j+1,] <- tmp
# }

dat.sel <- matrix(0, T, k)
dat.sel[1,] <- pre.data[1,]
X.tmp <- pre.data[1:(T-1),]
Y.tmp <- pre.data[2:T,]
dat.sel[2:T,] <- Y.tmp - X.tmp %*% t(L.new)

### Apply sparse fused-lasso to estimate change point candidates by using the removed low-rank data
temp.first <- first.step.cv.new.blocks.rank("LASSO", dat.sel, weight = NULL, lambda.1.cv, 1, max.iteration = max.iteration, 
                                            tol = tol, step.size = (1/2)*10^(-4), cv.index, blocks = blocks, initial = initial.phi)
print(temp.first$cv)
fisrt.brk.points <- temp.first$brk.points
pts <- fisrt.brk.points
print(fisrt.brk.points)

###### Step 2 ######

# dat.screen <- matrix(0, T, k); dat.screen[1,] <- pre.data[1,]
# Y.tmp <- matrix(0, T-1, k); Y.tmp <- pre.data[(2:T),]
# for(j in 1:(T-1)){
#     xtm <- matrix(dat.screen[j,], 1, k)
#     ytm <- matrix(Y.tmp[j,], 1, k)
#     tmp <- ytm - xtm %*% t(L.new)
#     dat.screen[j+1,] <- tmp
# }

dat.screen <- matrix(0, T, k); dat.screen[1,] <- pre.data[1,]
X.tmp <- pre.data[1:(T-1),]
Y.tmp <- pre.data[2:T,]
dat.screen[-1,] <- Y.tmp - X.tmp %*% t(L.new)

n <- T - 1
## 1/40 for 1-factor
## 1/20 for 3-factor
## 1/15 for 5-factor
omega <- (1/20)*(((log(n))^1)*log(k))  ### the penalty term in the information criterion: the higher omega, the smaller number of break points selected.
lambda.2 <- (1/2)*(log(n)*log(k))/n    ### the second tuning parameter.
temp <- second.step(dat.screen, lambda = lambda.2, p, max.iteration = 200, tol = tol, step.size = 10^(-3), pts, omega)
final.brk.points <- temp$pts           ### final break points selected

MTSplot(pre.data)
abline(v = final.brk.points)

###### Step 3 ######
### Estimated change points
pts.final <- final.brk.points
pts.final <- c(56, 84, 112, 336, 364, 392, 420, 504, 672, 756)
m.hat <- length(pts.final)
pts.est <- rep(0, m.hat+2)
pts.est[1] <- 0
pts.est[m.hat+2] <- T
pts.est[2:(m.hat+1)] <- pts.final

### Partition dataset according to the estimated change points
data.seg <- vector("list", m.hat+1)
for(j in 1:(m.hat+1)){
    data.seg[[j]] <- as.matrix(pre.data[((pts.est[j]+1):pts.est[j+1]),])
}

###### Initialize parameters
L.hat <- L.new
S.hat <- vector("list", m.hat+1)
for(j in 1:(m.hat+1)){ 
    S.hat[[j]] <- 0*diag(k)
}

###### Starting Algorithm according to Theorem 4. (simplified version) ######
# loss.val <- 0
# loss.val.new <- 1e5
# rr <- 1
# lambda.sel <- rep(0, m.hat+1)
# obj.vals <- c()

### Tuning parameters
#(old version low-rank)
# mu.sel <- 0.15 ### for 1-factor model
# mu.sel <- 0.355 ### for 3-factor model.
# mu.sel <- 0.5 ### for 5-factor model.


## sparse tunings 
lambda.vec <- vector("list", m.hat+1)

### for 1-factor:
# lambda.vec[[1]] <- seq(0.0075, 0.015, length.out = 5)
# lambda.vec[[2]] <- seq(0.005, 0.01, length.out = 5)
# lambda.vec[[3]] <- seq(0.0025, 0.005, length.out = 5)

### for 3-factor: (old version)
# lambda.vec[[1]] <- seq(0.0075, 0.01, length.out = 5)
# lambda.vec[[2]] <- seq(0.005, 0.01, length.out = 5)
# lambda.vec[[3]] <- seq(0.0025, 0.005, length.out = 5)
# lambda.vec[[4]] <- seq(0.005, 0.01, length.out = 5)
# lambda.vec[[5]] <- seq(0.005, 0.0075, length.out = 5)
# lambda.vec[[6]] <- seq(0.0075, 0.025, length.out = 5)
# lambda.vec[[7]] <- seq(0.01, 0.025, length.out = 5)
# lambda.vec[[8]] <- seq(0.005, 0.0075, length.out = 5)
# lambda.vec[[9]] <- seq(0.005, 0.0075, length.out = 5)
# lambda.vec[[10]] <- seq(0.0025, 0.005, length.out = 5)
# lambda.vec[[11]] <- seq(0.0025, 0.005, length.out = 5)

### for 3-factor: (simplified version)
lambda.vec[[1]] <- seq(0.01, 0.025, length.out = 5)
lambda.vec[[2]] <- seq(0.01, 0.025, length.out = 5)
lambda.vec[[3]] <- seq(0.0025, 0.0075, length.out = 5)
lambda.vec[[4]] <- seq(0.01, 0.05, length.out = 5)
lambda.vec[[5]] <- seq(0.0025, 0.0075, length.out = 5)
lambda.vec[[6]] <- seq(0.0025, 0.0075, length.out = 5)
lambda.vec[[7]] <- seq(0.01, 0.025, length.out = 5)
lambda.vec[[8]] <- seq(0.01, 0.05, length.out = 5)
lambda.vec[[9]] <- seq(0.0075, 0.025, length.out = 5)
lambda.vec[[10]] <- seq(0.005, 0.0075, length.out = 5)
lambda.vec[[11]] <- seq(0.005, 0.01, length.out = 5)

### for 5-factor: 
lambda.vec[[1]] <- seq(0.01, 0.05, length.out = 5)
lambda.vec[[2]] <- seq(0.01, 0.05, length.out = 5)
lambda.vec[[3]] <- seq(0.0025, 0.0075, length.out = 5)
lambda.vec[[4]] <- seq(0.01, 0.05, length.out = 5)
lambda.vec[[5]] <- seq(0.0025, 0.0075, length.out = 5)
lambda.vec[[6]] <- seq(0.0025, 0.0075, length.out = 5)
lambda.vec[[7]] <- seq(0.01, 0.025, length.out = 5)
lambda.vec[[8]] <- seq(0.01, 0.05, length.out = 5)
lambda.vec[[9]] <- seq(0.005, 0.025, length.out = 5)
lambda.vec[[10]] <- seq(0.005, 0.01, length.out = 5)
lambda.vec[[11]] <- seq(0.005, 0.01, length.out = 5)

### Step 0: Use initial low-rank component to initialize sparse components for each segments
for(j in 1:(m.hat+1)){
    len <- dim(data.seg[[j]])[1]
    X.seg <- data.seg[[j]][1:(len-1),]
    Y.seg <- data.seg[[j]][2:len,] - X.seg %*% t(L.hat)            #### use removed data
    fit.sparse <- bic.sparse(X.seg, Y.seg, lambda.vec[[j]], L.hat)
    S.hat[[j]] <- fit.sparse$S.hat
    print(paste("Estimating segment:", j, "...... with tuning", fit.sparse$lambda))
}

# while(abs(loss.val.new - loss.val) > tol){
#     if(rr > max.iter){
#         break
#     }else{
#         print(paste(as.integer(rr), loss.val.new))
#         rr <- rr + 1
#         loss.val <- loss.val.new
#         obj.vals <- c(obj.vals, loss.val)
#         ### Step 1: Use estimated (initialized) sparse component to re-estimate low-rank component
#         ### Re-build dataset by removing sparse components
#         # Y.seg <- vector("list", m.hat+1)
#         # data.new.seg <- vector("list", m.hat+1)
#         # for(j in 1:(m.hat+1)){
#         #     data.new.seg[[j]] = Y.seg[[j]] <- matrix(0, dim(data.seg[[j]])[1], dim(data.seg[[j]])[2])
#         #     data.new.seg[[j]][1,] <- data.seg[[j]][1,]
#         #     Y.seg[[j]] <- data.seg[[j]][-1,]
#         #     for(jj in 1:(dim(data.seg[[j]])[1]-1)){
#         #         xtm <- matrix(data.new.seg[[j]][jj,], 1, k)
#         #         ytm <- matrix(Y.seg[[j]][jj,], 1, k)
#         #         tmp <- ytm - xtm %*% t(S.hat[[j]])
#         #         data.new.seg[[j]][jj+1,] <- tmp
#         #     }
#         # }
#         
#         data.lr = X.tmp = Y.tmp <- vector("list", m.hat+1)
#         for(jj in 1:(m.hat+1)){
#             len <- dim(data.seg[[jj]])[1]
#             data.lr[[jj]] <- matrix(0, dim(data.seg[[jj]])[1], dim(data.seg[[jj]])[2])
#             data.lr[[jj]][1,] <- data.seg[[jj]][1,]
#             X.tmp[[jj]] <- data.seg[[jj]][1:(len-1),]
#             Y.tmp[[jj]] <- data.seg[[jj]][2:len,]
#             data.lr[[jj]][-1,] <- Y.tmp[[jj]] - X.tmp[[jj]] %*% t(S.hat[[jj]])
#         }
#         
#         data.sparse <- c()
#         for(j in 1:(m.hat+1)){
#             data.sparse <- rbind(data.sparse, as.matrix(data.lr[[j]]))
#         }
#         
#         ## X.lr <- data.sparse[1:(T-1),]
#         X.lr <- pre.data[1:(T-1),]
#         Y.lr <- data.sparse[2:T,]
#         fit.L <- fista.nuclear(X.lr, Y.lr, lambda = mu.sel, k, niter = 25, backtracking = TRUE, 0*diag(k))
#         L.hat <- t(fit.L$phi.hat)
#         print(qr(L.hat)$rank)
#         
#         ### Step 2: Re-estimate sparse components until converge
#         
#         # dat.tmp <- matrix(0, T, k); dat.tmp[1,] <- pre.data[1,]
#         # Y.tmp <- matrix(0, T-1, k); Y.tmp <- pre.data[(2:T),]
#         # for(j in 1:(T-1)){
#         #     xtm <- matrix(dat.tmp[j,], 1, k)
#         #     ytm <- matrix(Y.tmp[j,], 1, k)
#         #     tmp <- ytm - xtm %*% t(L.hat)
#         #     dat.tmp[j+1,] <- tmp
#         # }
#         
#         # data.est <- vector("list", m.hat+1)
#         # X.sp = Y.sp <- vector("list", m.hat+1)
#         # for(j in 1:(m.hat+1)){
#         #     data.est[[j]] <- as.matrix(dat.tmp[((pts.est[j]+1):pts.est[j+1]),])
#         #     len <- dim(data.est[[j]])[1]
#         #     X.sp[[j]] <- data.est[[j]][1:(len-1),]
#         #     Y.sp[[j]] <- data.est[[j]][2:len,]
#         #     fit.sparse <- bic.sparse(X.sp[[j]], Y.sp[[j]], lambda.vec[[j]], L.hat)
#         #     S.hat[[j]] <- fit.sparse$S.hat
#         #     lambda.sel[j] <- fit.sparse$lambda
#         #     print(paste("Estimating segment:", j, "...... with tuning", fit.sparse$lambda))
#         # }
#         
#         for(j in 1:(m.hat+1)){
#             len <- dim(data.seg[[j]])[1]
#             X.seg <- data.seg[[j]][1:(len-1),]                            ### use original data as design matrix
#             Y.seg <- data.seg[[j]][2:len,] - X.seg %*% t(L.hat)           ### remove low-rank effects
#             fit.sparse <- bic.sparse(X.seg, Y.seg, lambda.vec[[j]], L.hat)
#             S.hat[[j]] <- fit.sparse$S.hat
#             lambda.sel[j] <- fit.sparse$lambda
#             print(paste("Estimating segment:", j, "...... with tuning", fit.sparse$lambda))
#         }
#         
#         ### stopping criterion values
#         loss.val.new <- obj.func(L.hat, S.hat, pre.data[1:(T-1),], pre.data[2:T,], lambda = lambda.sel[1], mu = mu.sel)
#         loss.val.new <- round(loss.val.new, 3)
#         print("===============================================")
#     }
# }

### residual analysis
phi.est <- vector("list", m.hat+1)
for(j in 1:(m.hat+1)){
    phi.est[[j]] <- L.hat + S.hat[[j]]
}

total_error <- 0
for(j in 1:(m.hat+1)){
    len <- dim(data.seg[[j]])[1]
    X.seg <- data.seg[[j]][1:(len-1),]
    error.seg <- data.seg[[j]][2:len,] - X.seg %*% t(phi.est[[j]])            #### use removed data
    total_error <- total_error + (1/len)*sum(error.seg^2)
}
print(total_error)



### perform networks
phi.est <- matrix(0, k, k*(m.hat+1))
for(j in 1:(m.hat+1)){
    phi.est[,((j-1)*k+1):(k*j)] <- as.matrix(L.hat + S.hat[[j]])
}

# print(plot.matrix(abs(phi.est),m.hat+1))
print(plot.matrix(abs(L.hat),1, name = "Low-rank"))
print(qr(L.hat)$rank)
# for(j in 1:(m.hat+1)){
#     print(plot.matrix(abs(S.hat[[j]]), 1, name = paste(j, "sparse")))
# }

### Plotting the structure of sparse components for each segments
library(igraph)
for(seg in 1:(m.hat+1)){
    if(seg %in% c(4, 7, 9)){
        adj.mat <- matrix(0, k, k)
        for(i in 1:k){
            for(j in 1:k){
                if(abs(S.hat[[seg]][i,j]) > 0.125){
                    adj.mat[i,j] <- sign(S.hat[[seg]][i,j])
                    #adj.mat[i,j] <- S.hat[[seg]][i,j]
                }
            }
        }
        colnames(adj.mat) <- colnames(pre.data)
        net <- graph.adjacency(adj.mat, "directed", diag = FALSE)
        print(paste(seg, "edges:", length(E(net)), "connectivity:", length(E(net)) / (k^2)))
        l <- layout_in_circle(net)
        plot.igraph(net, vertex.label = V(net)$name, layout = l, vertex.label.font = 2,
                    vertex.label.color = "black", edge.color = "gray40", edge.arrow.size = 0.1,
                    vertex.shape ="none", vertex.color = "orange", vertex.label.cex = 0.8)
    }
    
}
# print(time.stamp[final.brk.points])
print(time.stamp[pts.final])
